{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203609f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_helper import MobileNetV2, Inv2d\n",
    "import wandb\n",
    "import mysql.connector as connector\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import mobilenet_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a68709",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7233b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc526a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# research_dir = Path(home, 'Desktop', 'Education', 'Spring 2025', 'AI', 'research')\n",
    "research_dir = Path(home, \"ai_research_proj_spring_2025\", \"research_ai_class_spring_2025\") # in lab 409 computer for Agafia\n",
    "os.chdir(research_dir)\n",
    "\n",
    "from data_helper import SQLDataset_Informative\n",
    "\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "# transforms\n",
    "transformations = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True), \n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70622e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "home = os.path.expanduser('~')\n",
    "os.chdir(home) # b/c we will be using universal paths\n",
    "\n",
    "host = '127.0.0.1'\n",
    "user = 'root' # change to your username\n",
    "password = 'vasya1' # change to your password\n",
    "database = 'ai_proj_2025' # we should all have this as the db name \n",
    "\n",
    "try:\n",
    "    conn = connector.connect(\n",
    "        host = host, \n",
    "        user = user, \n",
    "        password = password, \n",
    "        database = database\n",
    "    )\n",
    "    print('success')\n",
    "except connector.Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcf7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val, and test sets\n",
    "\n",
    "data_dir=Path(home, 'OneDrive - Stephen F. Austin State University', 'CrisisMMD_v2.0','CrisisMMD_v2.0')\n",
    "\n",
    "train_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_train=True)\n",
    "val_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_val=True)\n",
    "test_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03f35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_2points = [train_set.__getitem__(i) for i in range(2)]\n",
    "val_set_2points = [val_set.__getitem__(i) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43793277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=256)\n",
    "val_loader = DataLoader(val_set, batch_size=128)\n",
    "test_loader = DataLoader(test_set, batch_size=128)\n",
    "\n",
    "# for data in train_loader:\n",
    "#     print(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0afbb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the convnet (Eventually replae with mobile 2)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3) #specify the size (outer numbers) and amount (middle number) of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2) #specify pool size first number is size of pool, second is step size\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3) #new depth is amount of filters in previous conv layer\n",
    "        self.fc1 = nn.Linear(54*54*8, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 2) #finial fc layer needs 19 outputs because we have 19 layers # ???\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "     \n",
    "        x = self.pool(x)\n",
    "       \n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 54*54*8) # flatten\n",
    "\n",
    "        x = F.relu(self.fc1(x))    #fully connected, relu         \n",
    "        x = F.relu(self.fc2(x))    \n",
    "       \n",
    "        x = self.fc3(x)     #output    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123fc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing accuracy metric functions\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b869c42",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Make sure to change the run_name (and 'architecture' parameter of the wandb `run` variable if necessary) with each new run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25851dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation fn\n",
    "from collections import defaultdict \n",
    "\n",
    "def dev(model, val_loader):\n",
    "    model.to(device)\n",
    "    batch_size = val_loader.batch_size\n",
    "    avg = 'macro' # used when computing certain accuracy metrics\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, batch in tqdm.tqdm(enumerate(val_loader), \n",
    "                             total= len(val_loader), desc=f\"Processing validation data\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            raw_logits = model.forward(images)\n",
    "\n",
    "            preds = torch.argmax(raw_logits, dim=1) # https://discuss.pytorch.org/t/cross-entropy-loss-get-predicted-class/58215\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_trues.extend(labels.tolist())\n",
    "\n",
    "\n",
    "        # metrics \n",
    "        acc_total = accuracy_score(y_true=all_trues, y_pred=all_preds)\n",
    "        precision = precision_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        recall = recall_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        f1 = f1_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "\n",
    "        avg_eval_loss = eval_loss / (len(val_loader))\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': acc_total, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1': f1, \n",
    "            'avg_eval_loss': avg_eval_loss\n",
    "        }\n",
    "        wandb.log(metrics)\n",
    "        print('****Evaluation****')\n",
    "        print(f'total_accuracy: {acc_total}')\n",
    "\n",
    "        return acc_total, zip(all_preds, all_trues)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c75d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, num_epochs, run_name, lr, architecture, frozen_layers, dataset='CrisisMMD'):\n",
    "    # training hyperparameters & functions/tools\n",
    "    lr = lr \n",
    "    num_epochs = num_epochs\n",
    "    run_name = run_name\n",
    "\n",
    "    misclassified = defaultdict(int)\n",
    "    \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #stocastic gradient descent for our optimization algorithm\n",
    "    lr_sched = MultiStepLR(optimizer=optimizer, milestones=list(range(50, num_epochs, 30)), gamma=.1)\n",
    "\n",
    "    model.to(device)\n",
    "    # for saving the models\n",
    "    Path(research_dir, 'models' ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # before training, set up wandb for tracking purposes\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"5a08d1ebbf0e86ab877a128b98be3c320301b6a0\"\n",
    "\n",
    "    run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"agafiabschool-stephen-f-austin-state-university\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"Research Project for CSCI-1465\",\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": \"CrisisMMD\",\n",
    "            \"epochs\": num_epochs,\n",
    "            'frozen_layers': frozen_layers,\n",
    "        }, name=run_name\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        wandb.log({'epoch': epoch+1})\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for b, batch in tqdm.tqdm(enumerate(train_loader), \n",
    "                            total= len(train_loader), desc=f\"Processing training data in epoch {epoch+1}\"):\n",
    "            model.train()\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            model.zero_grad() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            raw_logits = model.forward(images)\n",
    "            # loss - can use raw logits for this function b/c it applies LogSoftmax \n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "            print(f'Train Loss: {loss}')\n",
    "            wandb.log({'Train Loss': loss.item()})\n",
    "            wandb.log({'LR': lr_sched.get_last_lr()[0]})\n",
    "\n",
    "\n",
    "            # backprop!\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b+1) % 20 == 0:\n",
    "                print(f'batch: {b+1} ; loss: {loss.item()}')\n",
    "        \n",
    "        # each epoch, run validation\n",
    "        acc, zipped = dev(model=model, val_loader=val_loader)\n",
    "\n",
    "        for i, (pred, true) in enumerate(zipped):\n",
    "            if true != pred: \n",
    "                misclassified[i] += 1\n",
    "        \n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            torch.save(model, Path(research_dir, 'models', f'{run_name}'))\n",
    "        \n",
    "        lr_sched.step()\n",
    "    \n",
    "    return best_val_acc, misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b5867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: agafiabschool (agafiabschool-stephen-f-austin-state-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\bowdenaa\\wandb\\run-20250506_212559-y0llvqhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465/runs/y0llvqhl' target=\"_blank\">Basic CNN test -- lr=0.001</a></strong> to <a href='https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465' target=\"_blank\">https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465/runs/y0llvqhl' target=\"_blank\">https://wandb.ai/agafiabschool-stephen-f-austin-state-university/Research%20Project%20for%20CSCI-1465/runs/y0llvqhl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.685461163520813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   2%|▏         | 1/64 [00:01<01:16,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1263749599456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   3%|▎         | 2/64 [00:02<01:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.841259241104126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   5%|▍         | 3/64 [00:03<01:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6574533581733704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   6%|▋         | 4/64 [00:04<00:58,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.713516891002655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   8%|▊         | 5/64 [00:04<00:54,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7470666170120239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:   9%|▉         | 6/64 [00:05<00:51,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7539517283439636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  11%|█         | 7/64 [00:06<00:54,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6862316727638245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  12%|█▎        | 8/64 [00:07<00:52,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.674427330493927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  14%|█▍        | 9/64 [00:08<00:49,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6794255375862122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  16%|█▌        | 10/64 [00:09<00:48,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.694210410118103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  17%|█▋        | 11/64 [00:10<00:46,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.718181848526001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  19%|█▉        | 12/64 [00:11<00:45,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.712857723236084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  20%|██        | 13/64 [00:12<00:44,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7003750801086426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  22%|██▏       | 14/64 [00:12<00:43,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6950231790542603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  23%|██▎       | 15/64 [00:13<00:42,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7129763960838318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  25%|██▌       | 16/64 [00:14<00:42,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7091782689094543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  27%|██▋       | 17/64 [00:15<00:41,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6899487972259521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  28%|██▊       | 18/64 [00:16<00:40,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.681421160697937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  30%|██▉       | 19/64 [00:17<00:39,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6849035620689392\n",
      "batch: 20 ; loss: 0.6849035620689392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  31%|███▏      | 20/64 [00:18<00:40,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6821866631507874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  33%|███▎      | 21/64 [00:19<00:39,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7256664037704468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  34%|███▍      | 22/64 [00:20<00:37,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7083988785743713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  36%|███▌      | 23/64 [00:20<00:35,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6886200904846191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  38%|███▊      | 24/64 [00:21<00:34,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6926554441452026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  39%|███▉      | 25/64 [00:22<00:34,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7106735706329346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  41%|████      | 26/64 [00:23<00:35,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7117250561714172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  42%|████▏     | 27/64 [00:24<00:34,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7184265851974487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  44%|████▍     | 28/64 [00:25<00:33,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7273117303848267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  45%|████▌     | 29/64 [00:26<00:32,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7185964584350586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  47%|████▋     | 30/64 [00:27<00:31,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7185862064361572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  48%|████▊     | 31/64 [00:28<00:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6959285736083984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  50%|█████     | 32/64 [00:29<00:30,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.692748486995697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  52%|█████▏    | 33/64 [00:30<00:29,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6889830827713013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  53%|█████▎    | 34/64 [00:31<00:28,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6827639937400818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  55%|█████▍    | 35/64 [00:32<00:28,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6859844923019409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  56%|█████▋    | 36/64 [00:33<00:27,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6790741682052612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  58%|█████▊    | 37/64 [00:34<00:26,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.762104332447052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  59%|█████▉    | 38/64 [00:35<00:24,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7488265037536621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  61%|██████    | 39/64 [00:35<00:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7500255107879639\n",
      "batch: 40 ; loss: 0.7500255107879639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  62%|██████▎   | 40/64 [00:36<00:22,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7408356070518494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  64%|██████▍   | 41/64 [00:37<00:21,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7126501798629761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  66%|██████▌   | 42/64 [00:38<00:19,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7002103328704834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  67%|██████▋   | 43/64 [00:39<00:18,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.694526731967926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  69%|██████▉   | 44/64 [00:40<00:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6907820105552673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  70%|███████   | 45/64 [00:41<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6943063735961914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  72%|███████▏  | 46/64 [00:42<00:16,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6990026235580444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  73%|███████▎  | 47/64 [00:43<00:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6975648999214172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  75%|███████▌  | 48/64 [00:43<00:14,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.700809895992279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  77%|███████▋  | 49/64 [00:44<00:13,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6992950439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  78%|███████▊  | 50/64 [00:45<00:12,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7023670673370361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  80%|███████▉  | 51/64 [00:46<00:11,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7037017345428467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  81%|████████▏ | 52/64 [00:47<00:10,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6943080425262451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  83%|████████▎ | 53/64 [00:48<00:09,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6881586313247681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  84%|████████▍ | 54/64 [00:49<00:08,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6882871389389038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  86%|████████▌ | 55/64 [00:49<00:07,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6885460615158081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  88%|████████▊ | 56/64 [00:50<00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6876941323280334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  89%|████████▉ | 57/64 [00:51<00:06,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6876626014709473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  91%|█████████ | 58/64 [00:52<00:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.692563533782959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  92%|█████████▏| 59/64 [00:53<00:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6915305852890015\n",
      "batch: 60 ; loss: 0.6915305852890015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  94%|█████████▍| 60/64 [00:54<00:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.689386248588562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  95%|█████████▌| 61/64 [00:55<00:02,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6981763243675232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  97%|█████████▋| 62/64 [00:55<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7030037045478821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1:  98%|█████████▊| 63/64 [00:56<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7077253460884094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 1: 100%|██████████| 64/64 [00:57<00:00,  1.12it/s]\n",
      "Processing validation data: 100%|██████████| 8/8 [00:03<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Evaluation****\n",
      "total_accuracy: 0.5542035398230089\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6966164112091064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   2%|▏         | 1/64 [00:00<01:01,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6951183676719666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   3%|▎         | 2/64 [00:01<00:56,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.694015383720398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   5%|▍         | 3/64 [00:02<00:51,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6946074962615967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   6%|▋         | 4/64 [00:03<00:50,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.694968044757843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   8%|▊         | 5/64 [00:04<00:48,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.693414568901062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:   9%|▉         | 6/64 [00:05<00:48,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6944633722305298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  11%|█         | 7/64 [00:05<00:48,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6923880577087402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  12%|█▎        | 8/64 [00:06<00:47,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6900097131729126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  14%|█▍        | 9/64 [00:07<00:46,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6893888711929321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  16%|█▌        | 10/64 [00:08<00:44,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6909520030021667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  17%|█▋        | 11/64 [00:09<00:44,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6899014711380005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  19%|█▉        | 12/64 [00:10<00:42,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6899839639663696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  20%|██        | 13/64 [00:10<00:42,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.692211925983429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  22%|██▏       | 14/64 [00:11<00:42,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6961523294448853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  23%|██▎       | 15/64 [00:12<00:41,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6864972114562988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  25%|██▌       | 16/64 [00:13<00:41,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6892929077148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  27%|██▋       | 17/64 [00:14<00:40,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6895900368690491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  28%|██▊       | 18/64 [00:15<00:38,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6921069025993347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  30%|██▉       | 19/64 [00:16<00:37,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.695046603679657\n",
      "batch: 20 ; loss: 0.695046603679657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  31%|███▏      | 20/64 [00:17<00:38,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6947031021118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  33%|███▎      | 21/64 [00:17<00:37,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6847718358039856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  34%|███▍      | 22/64 [00:18<00:35,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.677966833114624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  36%|███▌      | 23/64 [00:19<00:33,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6867771148681641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  38%|███▊      | 24/64 [00:20<00:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6884025931358337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  39%|███▉      | 25/64 [00:21<00:33,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6910014152526855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  41%|████      | 26/64 [00:22<00:33,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6929228901863098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  42%|████▏     | 27/64 [00:22<00:32,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6930840015411377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  44%|████▍     | 28/64 [00:23<00:31,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6979994773864746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  45%|████▌     | 29/64 [00:24<00:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6966859102249146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  47%|████▋     | 30/64 [00:25<00:29,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7018151879310608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  48%|████▊     | 31/64 [00:26<00:28,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6921390891075134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  50%|█████     | 32/64 [00:27<00:28,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6955468058586121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  52%|█████▏    | 33/64 [00:28<00:27,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6937297582626343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  53%|█████▎    | 34/64 [00:29<00:25,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6977639198303223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  55%|█████▍    | 35/64 [00:29<00:25,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7006604671478271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  56%|█████▋    | 36/64 [00:30<00:24,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6980662941932678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  58%|█████▊    | 37/64 [00:31<00:23,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.668484091758728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  59%|█████▉    | 38/64 [00:32<00:22,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6679157614707947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  61%|██████    | 39/64 [00:33<00:20,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6649847626686096\n",
      "batch: 40 ; loss: 0.6649847626686096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  62%|██████▎   | 40/64 [00:34<00:20,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.66912841796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  64%|██████▍   | 41/64 [00:35<00:19,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6645679473876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  66%|██████▌   | 42/64 [00:35<00:18,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6703693866729736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  67%|██████▋   | 43/64 [00:36<00:17,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6764172315597534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  69%|██████▉   | 44/64 [00:37<00:16,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7121334075927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  70%|███████   | 45/64 [00:38<00:16,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7163678407669067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  72%|███████▏  | 46/64 [00:39<00:15,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7296193838119507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  73%|███████▎  | 47/64 [00:40<00:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.72037273645401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  75%|███████▌  | 48/64 [00:40<00:13,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7145230174064636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  77%|███████▋  | 49/64 [00:41<00:12,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7030789256095886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  78%|███████▊  | 50/64 [00:42<00:11,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7102729678153992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  80%|███████▉  | 51/64 [00:43<00:10,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7044230103492737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  81%|████████▏ | 52/64 [00:44<00:10,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6959487199783325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  83%|████████▎ | 53/64 [00:45<00:09,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6875272989273071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  84%|████████▍ | 54/64 [00:45<00:08,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6891255378723145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  86%|████████▌ | 55/64 [00:46<00:07,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6917039752006531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  88%|████████▊ | 56/64 [00:47<00:06,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.690819263458252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  89%|████████▉ | 57/64 [00:48<00:05,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6898791193962097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  91%|█████████ | 58/64 [00:49<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6912925839424133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  92%|█████████▏| 59/64 [00:49<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6885873675346375\n",
      "batch: 60 ; loss: 0.6885873675346375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  94%|█████████▍| 60/64 [00:50<00:03,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6873135566711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  95%|█████████▌| 61/64 [00:51<00:02,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.690718412399292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  97%|█████████▋| 62/64 [00:52<00:01,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6916154623031616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2:  98%|█████████▊| 63/64 [00:53<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6904694437980652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 2: 100%|██████████| 64/64 [00:53<00:00,  1.19it/s]\n",
      "Processing validation data: 100%|██████████| 8/8 [00:03<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Evaluation****\n",
      "total_accuracy: 0.5420353982300885\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.707499086856842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   2%|▏         | 1/64 [00:00<01:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7049330472946167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   3%|▎         | 2/64 [00:01<00:53,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.699703574180603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   5%|▍         | 3/64 [00:02<00:50,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6999161839485168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   6%|▋         | 4/64 [00:03<00:49,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6934714317321777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training data in epoch 3:   8%|▊         | 5/64 [00:04<00:48,  1.22it/s]"
     ]
    }
   ],
   "source": [
    "# time to train\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "os.chdir(home)\n",
    "\n",
    "# for grid-search\n",
    "history = []\n",
    "# for lr in [10**-4, 30**-4, 10**-3, 30**-3, 10**-2, 30**-2, 10**-1, 30**-1, 1]:\n",
    "for lr in [10**-3]:\n",
    "    # run_name = f'MobileNetV2 lr={lr}'\n",
    "    run_name = f'Basic CNN test -- lr={lr}'\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    # instantiate our model\n",
    "    model = ConvNet()\n",
    "\n",
    "        \n",
    "    acc, misclassified = train_eval(model, num_epochs, run_name, lr=lr, architecture='Basic CNN', frozen_layers=0, dataset='CrisisMMD')\n",
    "    my_dict['acc'] = acc\n",
    "    my_dict['missed'] = misclassified\n",
    "    history.append(my_dict)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203174d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a940a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 1,\n",
       "             1: 1,\n",
       "             2: 1,\n",
       "             3: 1,\n",
       "             4: 1,\n",
       "             5: 1,\n",
       "             6: 1,\n",
       "             9: 1,\n",
       "             10: 1,\n",
       "             12: 1,\n",
       "             13: 1,\n",
       "             14: 1,\n",
       "             15: 1,\n",
       "             16: 1,\n",
       "             17: 1,\n",
       "             18: 1,\n",
       "             19: 1,\n",
       "             20: 1,\n",
       "             22: 1,\n",
       "             23: 1,\n",
       "             24: 1,\n",
       "             25: 1,\n",
       "             29: 1,\n",
       "             30: 1,\n",
       "             31: 1,\n",
       "             32: 1,\n",
       "             33: 1,\n",
       "             34: 1,\n",
       "             35: 1,\n",
       "             37: 1,\n",
       "             38: 1,\n",
       "             39: 1,\n",
       "             42: 1,\n",
       "             43: 1,\n",
       "             44: 1,\n",
       "             45: 1,\n",
       "             46: 1,\n",
       "             47: 1,\n",
       "             48: 1,\n",
       "             50: 1,\n",
       "             52: 1,\n",
       "             53: 1,\n",
       "             54: 1,\n",
       "             56: 1,\n",
       "             57: 1,\n",
       "             59: 1,\n",
       "             61: 1,\n",
       "             63: 1,\n",
       "             64: 1,\n",
       "             66: 1,\n",
       "             67: 1,\n",
       "             69: 1,\n",
       "             70: 1,\n",
       "             72: 1,\n",
       "             76: 1,\n",
       "             79: 1,\n",
       "             80: 1,\n",
       "             81: 1,\n",
       "             82: 1,\n",
       "             83: 1,\n",
       "             84: 1,\n",
       "             85: 1,\n",
       "             86: 1,\n",
       "             87: 1,\n",
       "             88: 1,\n",
       "             89: 1,\n",
       "             90: 1,\n",
       "             91: 1,\n",
       "             93: 1,\n",
       "             94: 1,\n",
       "             96: 1,\n",
       "             97: 1,\n",
       "             98: 1,\n",
       "             99: 1,\n",
       "             101: 1,\n",
       "             104: 1,\n",
       "             105: 1,\n",
       "             109: 1,\n",
       "             110: 1,\n",
       "             111: 1,\n",
       "             113: 1,\n",
       "             116: 1,\n",
       "             117: 1,\n",
       "             118: 1,\n",
       "             119: 1,\n",
       "             122: 1,\n",
       "             123: 1,\n",
       "             127: 1,\n",
       "             128: 1,\n",
       "             131: 1,\n",
       "             134: 1,\n",
       "             135: 1,\n",
       "             137: 1,\n",
       "             138: 1,\n",
       "             140: 1,\n",
       "             148: 1,\n",
       "             149: 1,\n",
       "             150: 1,\n",
       "             152: 1,\n",
       "             153: 1,\n",
       "             154: 1,\n",
       "             155: 1,\n",
       "             156: 1,\n",
       "             157: 1,\n",
       "             158: 1,\n",
       "             159: 1,\n",
       "             164: 1,\n",
       "             165: 1,\n",
       "             168: 1,\n",
       "             169: 1,\n",
       "             172: 1,\n",
       "             173: 1,\n",
       "             175: 1,\n",
       "             176: 1,\n",
       "             177: 1,\n",
       "             178: 1,\n",
       "             179: 1,\n",
       "             180: 1,\n",
       "             182: 1,\n",
       "             184: 1,\n",
       "             185: 1,\n",
       "             188: 1,\n",
       "             191: 1,\n",
       "             192: 1,\n",
       "             193: 1,\n",
       "             197: 1,\n",
       "             199: 1,\n",
       "             200: 1,\n",
       "             201: 1,\n",
       "             202: 1,\n",
       "             204: 1,\n",
       "             209: 1,\n",
       "             211: 1,\n",
       "             212: 1,\n",
       "             213: 1,\n",
       "             215: 1,\n",
       "             217: 1,\n",
       "             218: 1,\n",
       "             220: 1,\n",
       "             221: 1,\n",
       "             222: 1,\n",
       "             223: 1,\n",
       "             224: 1,\n",
       "             225: 1,\n",
       "             226: 1,\n",
       "             227: 1,\n",
       "             228: 1,\n",
       "             229: 1,\n",
       "             230: 1,\n",
       "             233: 1,\n",
       "             234: 1,\n",
       "             235: 1,\n",
       "             236: 1,\n",
       "             237: 1,\n",
       "             238: 1,\n",
       "             240: 1,\n",
       "             241: 1,\n",
       "             242: 1,\n",
       "             243: 1,\n",
       "             246: 1,\n",
       "             247: 1,\n",
       "             248: 1,\n",
       "             250: 1,\n",
       "             252: 1,\n",
       "             253: 1,\n",
       "             254: 1,\n",
       "             255: 1,\n",
       "             258: 1,\n",
       "             259: 1,\n",
       "             260: 1,\n",
       "             261: 1,\n",
       "             262: 1,\n",
       "             264: 1,\n",
       "             273: 1,\n",
       "             275: 1,\n",
       "             276: 1,\n",
       "             277: 1,\n",
       "             280: 1,\n",
       "             281: 1,\n",
       "             282: 1,\n",
       "             288: 1,\n",
       "             292: 1,\n",
       "             302: 1,\n",
       "             304: 1,\n",
       "             305: 1,\n",
       "             306: 1,\n",
       "             308: 1,\n",
       "             309: 1,\n",
       "             310: 1,\n",
       "             311: 1,\n",
       "             313: 1,\n",
       "             317: 1,\n",
       "             320: 1,\n",
       "             322: 1,\n",
       "             323: 1,\n",
       "             324: 1,\n",
       "             325: 1,\n",
       "             327: 1,\n",
       "             328: 1,\n",
       "             330: 1,\n",
       "             332: 1,\n",
       "             333: 1,\n",
       "             334: 1,\n",
       "             340: 1,\n",
       "             344: 1,\n",
       "             347: 1,\n",
       "             350: 1,\n",
       "             351: 1,\n",
       "             354: 1,\n",
       "             355: 1,\n",
       "             357: 1,\n",
       "             364: 1,\n",
       "             365: 1,\n",
       "             367: 1,\n",
       "             369: 1,\n",
       "             371: 1,\n",
       "             377: 1,\n",
       "             379: 1,\n",
       "             380: 1,\n",
       "             381: 1,\n",
       "             382: 1,\n",
       "             383: 1,\n",
       "             384: 1,\n",
       "             385: 1,\n",
       "             386: 1,\n",
       "             387: 1,\n",
       "             388: 1,\n",
       "             389: 1,\n",
       "             391: 1,\n",
       "             392: 1,\n",
       "             393: 1,\n",
       "             394: 1,\n",
       "             398: 1,\n",
       "             399: 1,\n",
       "             401: 1,\n",
       "             403: 1,\n",
       "             405: 1,\n",
       "             408: 1,\n",
       "             411: 1,\n",
       "             413: 1,\n",
       "             416: 1,\n",
       "             418: 1,\n",
       "             420: 1,\n",
       "             422: 1,\n",
       "             424: 1,\n",
       "             426: 1,\n",
       "             428: 1,\n",
       "             429: 1,\n",
       "             430: 1,\n",
       "             431: 1,\n",
       "             436: 1,\n",
       "             438: 1,\n",
       "             447: 1,\n",
       "             449: 1,\n",
       "             452: 1,\n",
       "             453: 1,\n",
       "             454: 1,\n",
       "             455: 1,\n",
       "             456: 1,\n",
       "             457: 1,\n",
       "             458: 1,\n",
       "             460: 1,\n",
       "             462: 1,\n",
       "             463: 1,\n",
       "             464: 1,\n",
       "             465: 1,\n",
       "             466: 1,\n",
       "             468: 1,\n",
       "             470: 1,\n",
       "             471: 1,\n",
       "             473: 1,\n",
       "             476: 1,\n",
       "             477: 1,\n",
       "             478: 1,\n",
       "             482: 1,\n",
       "             483: 1,\n",
       "             485: 1,\n",
       "             486: 1,\n",
       "             487: 1,\n",
       "             489: 1,\n",
       "             492: 1,\n",
       "             495: 1,\n",
       "             496: 1,\n",
       "             501: 1,\n",
       "             503: 1,\n",
       "             507: 1,\n",
       "             508: 1,\n",
       "             509: 1,\n",
       "             513: 1,\n",
       "             516: 1,\n",
       "             521: 1,\n",
       "             523: 1,\n",
       "             526: 1,\n",
       "             527: 1,\n",
       "             528: 1,\n",
       "             529: 1,\n",
       "             530: 1,\n",
       "             531: 1,\n",
       "             532: 1,\n",
       "             534: 1,\n",
       "             536: 1,\n",
       "             537: 1,\n",
       "             538: 1,\n",
       "             539: 1,\n",
       "             540: 1,\n",
       "             542: 1,\n",
       "             543: 1,\n",
       "             544: 1,\n",
       "             545: 1,\n",
       "             546: 1,\n",
       "             547: 1,\n",
       "             551: 1,\n",
       "             552: 1,\n",
       "             554: 1,\n",
       "             555: 1,\n",
       "             556: 1,\n",
       "             557: 1,\n",
       "             558: 1,\n",
       "             560: 1,\n",
       "             561: 1,\n",
       "             562: 1,\n",
       "             564: 1,\n",
       "             566: 1,\n",
       "             567: 1,\n",
       "             570: 1,\n",
       "             571: 1,\n",
       "             572: 1,\n",
       "             573: 1,\n",
       "             574: 1,\n",
       "             575: 1,\n",
       "             577: 1,\n",
       "             581: 1,\n",
       "             582: 1,\n",
       "             584: 1,\n",
       "             585: 1,\n",
       "             589: 1,\n",
       "             591: 1,\n",
       "             592: 1,\n",
       "             593: 1,\n",
       "             594: 1,\n",
       "             595: 1,\n",
       "             596: 1,\n",
       "             597: 1,\n",
       "             598: 1,\n",
       "             599: 1,\n",
       "             600: 1,\n",
       "             601: 1,\n",
       "             602: 1,\n",
       "             603: 1,\n",
       "             604: 1,\n",
       "             607: 1,\n",
       "             609: 1,\n",
       "             610: 1,\n",
       "             616: 1,\n",
       "             617: 1,\n",
       "             618: 1,\n",
       "             623: 1,\n",
       "             626: 1,\n",
       "             627: 1,\n",
       "             629: 1,\n",
       "             631: 1,\n",
       "             638: 1,\n",
       "             646: 1,\n",
       "             647: 1,\n",
       "             649: 1,\n",
       "             653: 1,\n",
       "             655: 1,\n",
       "             656: 1,\n",
       "             657: 1,\n",
       "             660: 1,\n",
       "             663: 1,\n",
       "             664: 1,\n",
       "             666: 1,\n",
       "             668: 1,\n",
       "             670: 1,\n",
       "             677: 1,\n",
       "             685: 1,\n",
       "             691: 1,\n",
       "             692: 1,\n",
       "             694: 1,\n",
       "             701: 1,\n",
       "             702: 1,\n",
       "             703: 1,\n",
       "             704: 1,\n",
       "             708: 1,\n",
       "             709: 1,\n",
       "             712: 1,\n",
       "             718: 1,\n",
       "             720: 1,\n",
       "             721: 1,\n",
       "             723: 1,\n",
       "             726: 1,\n",
       "             732: 1,\n",
       "             733: 1,\n",
       "             736: 1,\n",
       "             737: 1,\n",
       "             741: 1,\n",
       "             742: 1,\n",
       "             743: 1,\n",
       "             744: 1,\n",
       "             754: 1,\n",
       "             755: 1,\n",
       "             756: 1,\n",
       "             757: 1,\n",
       "             758: 1,\n",
       "             759: 1,\n",
       "             760: 1,\n",
       "             762: 1,\n",
       "             764: 1,\n",
       "             765: 1,\n",
       "             770: 1,\n",
       "             771: 1,\n",
       "             777: 1,\n",
       "             779: 1,\n",
       "             780: 1,\n",
       "             782: 1,\n",
       "             783: 1,\n",
       "             784: 1,\n",
       "             786: 1,\n",
       "             787: 1,\n",
       "             789: 1,\n",
       "             790: 1,\n",
       "             791: 1,\n",
       "             792: 1,\n",
       "             794: 1,\n",
       "             796: 1,\n",
       "             800: 1,\n",
       "             802: 1,\n",
       "             805: 1,\n",
       "             806: 1,\n",
       "             809: 1,\n",
       "             810: 1,\n",
       "             812: 1,\n",
       "             814: 1,\n",
       "             815: 1,\n",
       "             816: 1,\n",
       "             817: 1,\n",
       "             818: 1,\n",
       "             819: 1,\n",
       "             820: 1,\n",
       "             821: 1,\n",
       "             822: 1,\n",
       "             823: 1,\n",
       "             824: 1,\n",
       "             825: 1,\n",
       "             826: 1,\n",
       "             830: 1,\n",
       "             831: 1,\n",
       "             835: 1,\n",
       "             837: 1,\n",
       "             840: 1,\n",
       "             843: 1,\n",
       "             844: 1,\n",
       "             845: 1,\n",
       "             846: 1,\n",
       "             847: 1,\n",
       "             848: 1,\n",
       "             849: 1,\n",
       "             850: 1,\n",
       "             852: 1,\n",
       "             853: 1,\n",
       "             854: 1,\n",
       "             856: 1,\n",
       "             858: 1,\n",
       "             860: 1,\n",
       "             861: 1,\n",
       "             862: 1,\n",
       "             863: 1,\n",
       "             865: 1,\n",
       "             866: 1,\n",
       "             867: 1,\n",
       "             869: 1,\n",
       "             873: 1,\n",
       "             875: 1,\n",
       "             879: 1,\n",
       "             894: 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['missed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c4ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
