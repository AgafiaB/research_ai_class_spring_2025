{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51de915b",
   "metadata": {},
   "source": [
    "## Basic Convolution From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11752390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0112f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367b654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the filter\n",
    "l1_filter = np.random.randn(5,3,3,3) # 5 3D filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb217f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def conv_(imgs, filter):\n",
    "    assert(imgs.shape[0] == imgs.shape[1] and filter.shape[0] == filter.shape[1] and imgs.shape[-1] == filter.shape[-1])\n",
    "    \n",
    "    # remainding space - e.g. if remainder==2, then the first two columns of the filter will convolve with the last two columns of the image \n",
    "    remainder = imgs.shape[0] % filter.shape[0]\n",
    "    k = filter.shape[0]\n",
    "\n",
    "    feature_map = np.zeros((imgs.shape[0] - k + 1, imgs.shape[0] - k + 1))\n",
    "\n",
    "    for i in range(len(feature_map)):\n",
    "        f = copy.copy(filter)\n",
    "        for j in range(len(feature_map[1])):\n",
    "            feature = np.sum(np.multiply(imgs[i:i+k, j:j+k, :], f))\n",
    "            feature_map[i, j] = feature\n",
    "\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc414980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(imgs, conv_filter):\n",
    "    if len(imgs.shape) > 2 or len(conv_filter.shape) > 3: \n",
    "        if imgs.shape[-1] != conv_filter.shape[-1]:\n",
    "            print('ERROR: Number of channels (dimension [-1]) in the image and filter must match')\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: \n",
    "        print('ERROR: filter must be a square matrix. Dims [1] and [2] must match')\n",
    "    \n",
    "    if conv_filter.shape[-1] % 2 == 0:\n",
    "        print('ERROR: filter size must be odd')\n",
    "    \n",
    "    feature_maps = np.zeros((imgs.shape[0] - conv_filter.shape[1] + 1, \n",
    "                            imgs.shape[1] - conv_filter.shape[1] + 1, \n",
    "                            conv_filter.shape[0]))\n",
    "    \n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        curr_filter = conv_filter[filter_num, :] # get one filter from the stack of filters\n",
    "        conv_map = conv_(imgs, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map\n",
    "\n",
    "    return feature_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f40f23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.random.randn(222, 222, 3)\n",
    "test_filter = l1_filter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863afe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = conv(test_img, l1_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8bcb87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 220, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816e8cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfold = nn.Unfold(3, 1, 0, 1)\n",
    "item = torch.randn(2, 2, 3, 3) \n",
    "out = unfold(item)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea0d4c",
   "metadata": {},
   "source": [
    "## Basic Involution (using some Pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab0be68",
   "metadata": {},
   "source": [
    "```\n",
    "B: batch size, H: height, W: width\n",
    "C: channel number, G: group number\n",
    "K: kernel size, s: stride, r: reduction ratio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(imgs, r=1, kernel_size=3, group_ch=1, stride=1, padding=0):\n",
    "    '''\n",
    "    assumes imgs is a tensor of shape (batch_size, channels, w, h)\n",
    "    '''\n",
    "    \n",
    "    k = kernel_size\n",
    "    c_1 = imgs.shape[1]\n",
    "    g = c_1 // group_ch\n",
    "\n",
    "    o = nn.AvgPool2d(stride, stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "    # these conv layers are what will be changed w/ backprop\n",
    "    reduce = nn.Conv2d(c_1, c_1 // r, 1) \n",
    "    span = nn.Conv2d(c_1 // r, g*k*k, k, padding=padding) # is k supposed to be here??? \n",
    "    weights = (o(imgs))\n",
    "    weights = reduce(weights)\n",
    "    \n",
    "    weights = span(weights)\n",
    "    \n",
    "    b, c_2, h, w = weights.shape\n",
    "    \n",
    "    weights = weights.view(b, g, 1, k**2, w*h)\n",
    "    \n",
    "    patches = nn.Unfold(k, padding=padding)(imgs) # patches.shape = (b, c*k*k, L) where L is the number of times a 3x3 kernel can shift over an image\n",
    "    assert(patches.shape == (b, c_1*k*k, patches.shape[-1]))\n",
    "    \n",
    "    patches = patches.view(b, g, group_ch, k**2, w*h) # ( c_1 // group_ch) * group_ch = c_1  #  x_unfolded = x_unfolded.view(b, self.groups, self.group_ch, self.kernel_size**2, h, w)\n",
    "\n",
    "    out = (patches*weights) # out.shape = (b, g, g//c, k*k, w*h)\n",
    "\n",
    "    # out should be shape (b, c, h, w) \n",
    "    out = out.view(b, c_1, k*k, w*h) # we replace (g, g//c) with c because c=g*(g//c)\n",
    "    out = out.sum(dim=2) \n",
    "    out = out.view(b, c_1, w, h)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cb9495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 218, 218])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.randn(1, 3, 220, 220)\n",
    "\n",
    "\n",
    "test_out = inv(test_img)\n",
    "test_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1434366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
