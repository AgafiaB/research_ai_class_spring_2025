{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf50ab9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.ops import Conv2dNormActivation\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "import mysql.connector as connector\n",
    "from pathlib import Path\n",
    "import tqdm # progress bar\n",
    "\n",
    "\n",
    "# importing accuracy metric functions\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "os.chdir(home) # b/c we will be using universal paths\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ed3be",
   "metadata": {},
   "source": [
    "# Creating the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda35b4",
   "metadata": {},
   "source": [
    "The code for creating the database is provided in order to promote reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_text_table_query = '''\n",
    "CREATE Table Tweets(\n",
    "    tweet_id VARCHAR(50) PRIMARY KEY, \n",
    "    event VARCHAR(30), \n",
    "    text_info VARCHAR(50) NOT NULL,\n",
    "    text_info_conf DECIMAL(5,4),\n",
    "    text_human VARCHAR(50), \n",
    "    text_human_conf DECIMAL(5,4), \n",
    "    tweet_text VARCHAR(280)\n",
    ");\n",
    "'''\n",
    "create_image_table_query = '''\n",
    "CREATE TABLE Images (\n",
    "    idx int NOT NULL AUTO_INCREMENT PRIMARY KEY,\n",
    "    image_id VARCHAR(50) NOT NULL, \n",
    "    tweet_id VARCHAR(50),\n",
    "    FOREIGN KEY (tweet_id) REFERENCES Tweets(tweet_id),\n",
    "    image_path VARCHAR(200) NOT NULL, \n",
    "    image_info VARCHAR(50) NOT NULL, \n",
    "    image_info_conf DECIMAL(5,4), \n",
    "    image_human VARCHAR(50),\n",
    "    image_human_conf DECIMAL(5,4), \n",
    "    image_damage VARCHAR(50),\n",
    "    image_damage_conf DECIMAL(5,4), \n",
    "    image_url VARCHAR(100),\n",
    "    date DATETIME\n",
    ");\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e16d3",
   "metadata": {},
   "source": [
    "### Populating the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418be117",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(home_dir, ..., 'CrisisMMD_v2.0','CrisisMMD_v2.0') # change depending on where your data is located\n",
    "annot_path = data_path / 'annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66425ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_per_event = os.listdir(annot_path) # read all the file names in the folder\n",
    "annotations_per_event = [file_str for file_str in annotations_per_event if not file_str.startswith('._')] # the ._ files don't work for Windows OS\n",
    "annotations_per_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = data_path / 'data_image' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folders = os.listdir(image_dir_path)\n",
    "image_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d3a16",
   "metadata": {},
   "source": [
    "Now, we must create a function that can insert the data into our database. Let's break down what we will be inserting like so:\n",
    "\n",
    "1. From the annotations folder\n",
    "\n",
    "   This is the step where we access all information except for getting the actual image itself, which we will access in the next step using the image_path attribute we get from this step. \n",
    "   1. By event\n",
    "3. Getting the images\n",
    "   1. By event\n",
    "   2. By date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def populate_db():\n",
    "    for f in annotations_per_event: # annotations_per_event is the list of .tsv files\n",
    "        path = annot_path / f\n",
    "        df = pd.read_csv(path, sep='\\t') # a tsv file in the annotations folder for a specific event\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "\n",
    "        for (_, row) in df.iterrows(): # i is the idx, j is the series\n",
    "            \n",
    "            img_path = row.loc['image_path'] # will also use this to get the event name\n",
    "            date_pattern = r'(\\d{1,2}_\\d{1,2}_\\d{4})' # day month year - naming format of folders that the images are in \n",
    "            date = re.findall(date_pattern, img_path)[0]\n",
    "            \n",
    "            # PART 1 - Inserting into the Tweets table \n",
    "            event_pattern = r'data_image\\/(\\w+)\\/\\d{1,2}_\\d{1,2}_\\d{4}' \n",
    "            event = re.findall(event_pattern, img_path)[0]\n",
    "            \n",
    "            sql_text1 = \"\"\"\n",
    "            INSERT INTO Tweets (tweet_id, event, text_info, text_info_conf, text_human, text_human_conf, tweet_text) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            cursor.execute(f\"SELECT * FROM Tweets WHERE tweet_id='{row.loc['tweet_id']}'\")\n",
    "            if not cursor.fetchall():\n",
    "                cursor.execute(sql_text1, (\n",
    "                    row['tweet_id'], event, row['text_info'], row['text_info_conf'], row['text_human'],\n",
    "                    row['text_human_conf'], row['tweet_text']\n",
    "                ))\n",
    "            \n",
    "            # PART 2 - Inserting into the Images table\n",
    "            \n",
    "            cursor.execute(f\"SELECT * FROM Images WHERE image_id='{row.loc['image_id']}'\")\n",
    "            if not cursor.fetchall():\n",
    "                sql_img1 = sql_img1 = f\"\"\"\n",
    "                    INSERT INTO Images (image_id, tweet_id, image_path, image_info, image_info_conf, image_human, image_human_conf, image_damage, image_damage_conf, image_url, date) \n",
    "                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, {folder_to_date(date)})\n",
    "                    \"\"\"\n",
    "                cursor.execute(sql_img1, (\n",
    "                    row['image_id'], row['tweet_id'], row['image_path'], row['image_info'], row['image_info_conf'],\n",
    "                    row['image_human'], row['image_human_conf'], row['image_damage'], row['image_damage_conf'],\n",
    "                    row['image_url']\n",
    "                ))\n",
    "            \n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_db()\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9862f5",
   "metadata": {},
   "source": [
    "# Connecting to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c871263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dataset class to retrieve the data\n",
    "class SQLDataset_Informative(Dataset):\n",
    "    def __init__(self, conn, label_col, img_col='image_path', data_dir=Path(os.path.expanduser('~'), 'CrisisMMD_v2.0','CrisisMMD_v2.0'), \n",
    "                 transform=None, target_transform=None, is_train=False, is_test=False, is_val=False, table_name='Images'):\n",
    "        '''\n",
    "        Parameters: \n",
    "            conn - a mysql.connector object that will be used to retrieve a cursor \n",
    "            label_col - a name of type string that matches the column name in the sql database that labels the image data\n",
    "            img_col - a name of type string that matches the column name in the sql database that contains the image path\n",
    "            data_dir - the path that contains the folder containing the image paths in the sql database \n",
    "            transform - pytorch image transformations that transform the data\n",
    "            target_transform - does nothing as of now, so do not specify this\n",
    "            is_train | is_val | is_test - choose none or one of these; if none chosen, all data is used \n",
    "            table_name - a string that matches the table name; default: \"Images\"\n",
    "        \n",
    "        Notes:\n",
    "            is_train uses 90% of the data\n",
    "            is_val and is_test each use 5% of the data \n",
    "        '''\n",
    "        assert(not ((is_train and is_test) or (is_train and is_val) or (is_val and is_test)), 'a dataset can only be one of either train, test, or val')\n",
    "\n",
    "        self.conn = conn\n",
    "        self.img_col = img_col\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_dir = data_dir\n",
    "        self.is_train = is_train\n",
    "        self.is_val = is_val\n",
    "        self.is_test = is_test\n",
    "        self.table_name = table_name\n",
    "\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        # we need a list of available indices \n",
    "        # what idxs are available to use for this database? - depends on the dataset type\n",
    "        if not (self.is_train or self.is_val or self.is_test): # if no dataset type specified\n",
    "            query = 'SELECT COUNT(image_id) FROM ' + table_name\n",
    "            cursor.execute(query)\n",
    "            count = cursor.fetchone()[0]\n",
    "            self.possible_sql_idxs = range(count)\n",
    "        else:\n",
    "            \n",
    "            query = 'SELECT COUNT(image_id) FROM ' + table_name\n",
    "            cursor.execute(query)\n",
    "            count = cursor.fetchone()[0]\n",
    "            \n",
    "\n",
    "            # below, we use +1 because SQL indexing starts at 1\n",
    "            if is_train: \n",
    "                self.possible_sql_idxs = [i+1 for i in range(count) if (((i+1) % 20) < 18)]\n",
    "            elif is_val:\n",
    "                self.possible_sql_idxs = [i+1 for i in range(count) if (((i+1) % 20) == 18)]\n",
    "            else: # must be test\n",
    "                self.possible_sql_idxs = [i+1 for i in range(count) if (((i+1) % 20) > 18)]\n",
    "        cursor.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of images in the database \n",
    "        '''\n",
    "    \n",
    "\n",
    "        return len(self.possible_sql_idxs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Description: \n",
    "            Retrieves a tuple of (torch.tensor, string) where the first object is a 3D tensor of image data and the string is the label\n",
    "        '''\n",
    "        # retrieve an image from the sql database\n",
    "\n",
    "        cursor = self.conn.cursor()\n",
    "\n",
    "        try:\n",
    "                query = f'SELECT {self.img_col}, {self.label_col} FROM {self.table_name} WHERE idx={self.possible_sql_idxs[idx]}' \n",
    "                cursor.execute(query)\n",
    "                \n",
    "                # read in image\n",
    "                img_path, label = cursor.fetchone()\n",
    "                img_path = Path(self.data_dir, img_path)\n",
    "                image = decode_image(img_path, mode='RGB') # returns (Tensor[image_channels, image_height, image_width])\n",
    "\n",
    "                \n",
    "                if label == 'informative':\n",
    "                    label = torch.tensor(1)\n",
    "                else:\n",
    "                    label = torch.tensor(0)\n",
    "                # print(f'image shape before transform: {image.shape}')\n",
    "                # apply transforms on image \n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                if self.target_transform:\n",
    "                    label = self.target_transform(label)\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de89fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "user = 'root' # change to your username\n",
    "password = 'vasya1' # change to your password\n",
    "database = 'ai_proj_2025' # we should all have this as the db name \n",
    "\n",
    "try:\n",
    "    conn = connector.connect(\n",
    "        host = host, \n",
    "        user = user, \n",
    "        password = password, \n",
    "        database = database\n",
    "    )\n",
    "    print('success')\n",
    "except connector.Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c0773",
   "metadata": {},
   "source": [
    "# Creating and Transforming Datasets with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e12be9",
   "metadata": {},
   "source": [
    "We are utilizing the CrisisMMD dataset, which can be found here: https://crisisnlp.qcri.org/crisismmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51536ccd",
   "metadata": {},
   "source": [
    "This dataset includes images and text data from Twitter in 2017, a year of several major natural disasters across the globe. Some posts (images and/or text) are informative to humanitarian aid officials, while others may be irrelevant and distracting. The text and images were each labeled as informative or not informative based on human inference for supervised learning purposes. However, we are most interested in correctly classifying posted images as informative or not to ultimately pinpoint geographical areas that require the most or most urgent humanitarian aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444546d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up transforms\n",
    "transformations = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True), # resize all images to same size\n",
    "    v2.RandomHorizontalFlip(p=0.5), # add random changes in the image \n",
    "    v2.ToDtype(torch.float32, scale=True), \n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize channel-wise\n",
    "])\n",
    "\n",
    "data_dir=Path(home, 'OneDrive - Stephen F. Austin State University', 'CrisisMMD_v2.0','CrisisMMD_v2.0')\n",
    "\n",
    "train_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_train=True)\n",
    "val_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_val=True)\n",
    "test_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64) # load the data with torch\n",
    "val_loader = DataLoader(val_set, batch_size=128)\n",
    "test_loader = DataLoader(test_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9fd57",
   "metadata": {},
   "source": [
    "# First CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9c3fa",
   "metadata": {},
   "source": [
    "Since we are dealing with image data, we will implement a Convolutional Neural Network (CNN) to classify whether twitter images are informative in the context of a humanitarian crisis or disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3) # specify the size (outer numbers) and amount (middle number) of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2) # specify pool size first number is size of pool, second is step size\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3) # new depth is amount of filters in previous conv layer\n",
    "        self.fc1 = nn.Linear(54*54*8, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 2) # final fc layer needs 19 outputs because we have 19 layers # ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 54*54*8) # flatten\n",
    "        x = F.relu(self.fc1(x))    # fully connected, relu        \n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)     # output    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c87b",
   "metadata": {},
   "source": [
    "## Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loop\n",
    "def dev(model, val_loader): \n",
    "    model.to(device)\n",
    "    batch_size = val_loader.batch_size\n",
    "    avg = 'macro' # used when computing certain accuracy metrics\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, batch in tqdm.tqdm(enumerate(val_loader), \n",
    "                             total= len(val_loader), desc=f\"Processing validation data\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            raw_logits = model.forward(images)\n",
    "\n",
    "            preds = torch.argmax(raw_logits, dim=1) # https://discuss.pytorch.org/t/cross-entropy-loss-get-predicted-class/58215\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_trues.extend(labels.tolist())\n",
    "\n",
    "\n",
    "        # metrics \n",
    "        acc_total = accuracy_score(y_true=all_trues, y_pred=all_preds)\n",
    "        precision = precision_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        recall = recall_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        f1 = f1_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "\n",
    "        avg_eval_loss = eval_loss / (len(val_loader))\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': acc_total, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1': f1, \n",
    "            'avg_eval_loss': avg_eval_loss\n",
    "        }\n",
    "        print('****Evaluation****')\n",
    "        print(f'total_accuracy: {acc_total}')\n",
    "\n",
    "        return acc_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_eval(model, num_epochs, lr):\n",
    "    # training hyperparameters & functions/tools\n",
    "    lr = lr \n",
    "    num_epochs = num_epochs\n",
    "    \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #stocastic gradient descent for our optimization algorithm\n",
    "    lr_sched = MultiStepLR(optimizer=optimizer, milestones=list(range(50, num_epochs, 30)), gamma=.1) # decaying lr\n",
    "\n",
    "    model.to(device)\n",
    "    # for saving the models\n",
    "    #Path(research_dir, 'models' ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for b, batch in tqdm.tqdm(enumerate(train_loader), \n",
    "                            total= len(train_loader), desc=f\"Processing training data in epoch {epoch+1}\"):\n",
    "            model.train()\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            model.zero_grad() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            raw_logits = model.forward(images) # forward pass\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels) # raw logits as input\n",
    "            print(f'Train Loss: {loss}')\n",
    "            \n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # gradient update\n",
    "\n",
    "            if (b+1) % 20 == 0:\n",
    "                print(f'batch: {b+1} ; loss: {loss.item()}')\n",
    "        \n",
    "        # each epoch, run validation\n",
    "        acc = dev(model=model, val_loader=val_loader)\n",
    "        \n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            #torch.save(model, Path(research_dir, 'models', f'{run_name}'))\n",
    "        \n",
    "        lr_sched.step()\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f72e67",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "os.chdir(home)\n",
    "\n",
    "# grid-search\n",
    "history = []\n",
    "for lr in [10**-3, 10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    model = ConvNet()\n",
    "\n",
    "    acc = train_eval(model, num_epochs, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80795ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print/report final model hyperparameters and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b295cb",
   "metadata": {},
   "source": [
    "# Updated CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c68867",
   "metadata": {},
   "source": [
    "Since the previous CNN worked well, we will tweak that model architecture by adding layers to hopefully improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3) # specify the size (outer numbers) and amount (middle number) of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2) # specify pool size first number is size of pool, second is step size\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3) # new depth is amount of filters in previous conv layer\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*26*26, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 2) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3]) # flatten\n",
    "        x = F.relu(self.fc1(x))    # fully connected, relu         \n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573b71f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee04d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "os.chdir(home)\n",
    "\n",
    "history = []\n",
    "for lr in [10**-3, 10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    model = ConvNet2()\n",
    "\n",
    "    # update architecture = (?)\n",
    "    acc = train_eval(model, num_epochs, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d07d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0729b7",
   "metadata": {},
   "source": [
    "# MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48b269",
   "metadata": {},
   "source": [
    "With disaster reports being inherently spontaneous, we would like to investigate more efficient models such as MobileNetV2 to analyze the efficiency-accuracy tradeoff for predicting informative or not informative images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a494699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Involution Block - Filter for each \"location\" in the image\n",
    "class Inv2d(nn.Module): \n",
    "    def __init__(self, in_channels, reduction=1, kernel_size=3, group_ch=1, stride=1, padding=1, dilation=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.k = kernel_size\n",
    "        self.group_ch = group_ch\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.g = in_channels // group_ch\n",
    "        self.r = reduction\n",
    "\n",
    "        self.stride=stride\n",
    "        self.padding = padding\n",
    "        self.dilation=dilation\n",
    "\n",
    "        # avg pool is for adjusting the kernel and therefore output based on the stride\n",
    "        self.o = nn.AvgPool2d(kernel_size=stride, stride=stride) if stride > 1 else nn.Identity()\n",
    "        self.reduce = nn.Conv2d(in_channels, in_channels // reduction, 1)\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels//reduction)\n",
    "        self.span = nn.Conv2d(in_channels // reduction, self.g*(kernel_size**2), 1)\n",
    "        self.unfold = nn.Unfold(kernel_size, padding=padding, stride=stride, dilation=dilation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Input should be an image tensor of shape: (batch_size, channels, h, w)\n",
    "        '''\n",
    "        # KERNEL GENERATION\n",
    "        W = self.reduce(self.o(X))\n",
    "  \n",
    "        W = nn.ReLU()(self.batch_norm(W))\n",
    "        W = self.span(W)\n",
    " \n",
    "        b, c, h, w = W.shape\n",
    "        W = W.view(b, self.g, 1, self.k**2, w*h)\n",
    "\n",
    "        # INVOLUTION\n",
    "        # print(f'stride: {self.stride}')\n",
    "        # print(f'padding: {self.padding}')\n",
    "        \n",
    "        patches = self.unfold(X)\n",
    "        patches = patches.view(b, self.g, self.group_ch, self.k**2, w*h)\n",
    "        out = patches*W \n",
    "\n",
    "        # output width and height should be w / stride and h / stride\n",
    "        out = out.view(b, self.out_channels, self.k**2, w*h).sum(dim=2)\n",
    "        out = out.view(b, self.out_channels, w, h)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Inverted residual block - bottleneck architecture with residual connection (like resnet)\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio, norm_layer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert stride == 1 or stride == 2, 'stride must be 1 or 2'\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        hidden_dim = int(round(in_channels * expand_ratio))\n",
    "        # print(hidden_dim)\n",
    "\n",
    "        layers = [] \n",
    "\n",
    "        if expand_ratio != 1:\n",
    "            # add expansion \n",
    "            layers.append(Conv2dNormActivation(in_channels, hidden_dim, kernel_size=1, stride=stride, \n",
    "                                               norm_layer=norm_layer, activation_layer=nn.ReLU6))\n",
    "            # depthwise conv => pointwise => norm_layer\n",
    "            # hidden_dim == in_channels if expand_ratio==1\n",
    "            # b/c groups = hidden_dim, each input channel is convolved with its own set of filters\n",
    "            # b/c instead of using multiple 3d kernels, we want multiple 2d kernels that each go across a different channel\n",
    "        layers.extend([Conv2dNormActivation(hidden_dim, hidden_dim, kernel_size=3, stride=stride, groups=hidden_dim, norm_layer=norm_layer, activation_layer=nn.ReLU6), \n",
    "                                                nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False), # point-wise (1x1 filter)\n",
    "                                                norm_layer(out_channels)])\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self._is_cn = stride > 1 # downsample indicator\n",
    "        self.use_res_conn = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_conn:\n",
    "            return x + self.conv(x)\n",
    "        else:                \n",
    "            return self.conv(x)\n",
    "        \n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None): \n",
    "    '''\n",
    "    Description: rounds v to nearest multiple of divisor\n",
    "    '''\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# mobilenetv2 architecture\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "            block: Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer: Module specifying the normalization layer to use\n",
    "\n",
    "        \"\"\"\n",
    "    def __init__(self, width_mult=1.0, inverted_residual_setting = None, round_nearest=8,\n",
    "                  block = None, norm_layer = None, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        input_channel = 32 # adjustable\n",
    "        last_channel = 1280 # adjustable\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t - expansion factor, c - # of output after block, n - # of repitions, s - stride\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # build the first layer\n",
    "        # input_channel is the number of channels after the first Conv2dNormActivation\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        \n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        \n",
    "        features = [Conv2dNormActivation(3, input_channel, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.ReLU6)]\n",
    "\n",
    "        # build the inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c*width_mult, round_nearest) # update output channels (will increase)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
    "                input_channel = output_channel \n",
    "                \n",
    "        # build last severals\n",
    "        features.append(Conv2dNormActivation(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer, activation_layer=nn.ReLU6))\n",
    "\n",
    "        # complete CNN architecture\n",
    "        self.features = nn.Sequential(*features)  \n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            # print('initializing weights')\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def _forward_impl(self, x):\n",
    "        x = self.features(x) # pass input through layers\n",
    "        \n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)) # takes the average value for each each channel\n",
    "        x = torch.flatten(x, 1) # flatten for linear layer\n",
    "        # x = self.classifier(x) # classify\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a398b",
   "metadata": {},
   "source": [
    "## Transfer Learning with MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ae34b",
   "metadata": {},
   "source": [
    "We see if we can improve performance and/or efficiency of the MobileNetV2 model by using transfer learning with `weights` = 'IMAGENET1K_V2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "history = []\n",
    "for lr in [10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    # instantiate our model\n",
    "    model = MobileNetV2()\n",
    "\n",
    "    # load pretrained weights\n",
    "    pretrained = mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "    weights = pretrained.state_dict()\n",
    "    model.features.load_state_dict(weights, strict=False)\n",
    "\n",
    "    # turn off all but the topmost layers\n",
    "    freeze_up_to = 9\n",
    "    for param in model.features[:freeze_up_to].parameters(): \n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = nn.Sequential(model, nn.Linear(1280, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 2)) # 1280 is the num of features outputted by MobileNetv2 after flattening\n",
    "    \n",
    "    acc = train_eval(model, num_epochs, run_name, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final mobilenet metrics, is it better than the updated CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5db65",
   "metadata": {},
   "source": [
    "# Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b62709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad cam implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34b15a",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94407410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model choice and analysis on metrics (compared to baseline model or existing models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f525e32",
   "metadata": {},
   "source": [
    "# Contribution From Group Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db22f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stuff as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d45f",
   "metadata": {},
   "source": [
    "Agafia's Contribution: Implemented grid-search to optimize hyperparameters and transfer learning for MobileNetV2. Implemented involution class. Wrote code to connect to SQL server and build dataset in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fea653",
   "metadata": {},
   "source": [
    "Ethan's Contribution: Implemented and commented on ConvNet class and training/validation loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8713ec",
   "metadata": {},
   "source": [
    "Hudson's Contribution: Implemented and commented on MobileNetV2, InvertedResidual classes. Created final ipynb file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
