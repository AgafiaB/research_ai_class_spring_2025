{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf50ab9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.ops import Conv2dNormActivation\n",
    "\n",
    "from data_helper import SQLDataset_Informative\n",
    "import mysql.connector as connector\n",
    "from pathlib import Path\n",
    "import tqdm # progress bar\n",
    "import os\n",
    "\n",
    "# importing accuracy metric functions\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9862f5",
   "metadata": {},
   "source": [
    "# Connecting to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de89fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "os.chdir(home) # b/c we will be using universal paths\n",
    "\n",
    "host = '127.0.0.1'\n",
    "user = 'root' # change to your username\n",
    "password = 'ethan1' # change to your password\n",
    "database = 'ai_proj_2025' # we should all have this as the db name \n",
    "\n",
    "try:\n",
    "    conn = connector.connect(\n",
    "        host = host, \n",
    "        user = user, \n",
    "        password = password, \n",
    "        database = database\n",
    "    )\n",
    "    print('success')\n",
    "except connector.Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c0773",
   "metadata": {},
   "source": [
    "# Creating and Transforming Datasets with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e12be9",
   "metadata": {},
   "source": [
    "We are utilizing the CrisisMMD dataset, which can be found here: https://crisisnlp.qcri.org/crisismmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51536ccd",
   "metadata": {},
   "source": [
    "This dataset includes images and text data from Twitter in 2017, a year of several major natural disasters across the globe. Some posts (images and/or text) are informative to humanitarian aid officials, while others may be irrelevant and distracting. The text and images were each labeled as informative or not informative based on human inference for supervised learning purposes. However, we are most interested in correctly classifying posted images as informative or not to ultimately pinpoint geographical areas that require the most or most urgent humanitarian aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444546d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up transforms\n",
    "transformations = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True), # resize all images to same size\n",
    "    v2.RandomHorizontalFlip(p=0.5), # add random changes in the image \n",
    "    v2.ToDtype(torch.float32, scale=True), \n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalize channel-wise\n",
    "])\n",
    "\n",
    "data_dir=Path(home, 'OneDrive - Stephen F. Austin State University', 'CrisisMMD_v2.0','CrisisMMD_v2.0')\n",
    "\n",
    "train_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_train=True)\n",
    "val_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_val=True)\n",
    "test_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=256) # load the data with torch\n",
    "val_loader = DataLoader(val_set, batch_size=128)\n",
    "test_loader = DataLoader(test_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9fd57",
   "metadata": {},
   "source": [
    "# First CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9c3fa",
   "metadata": {},
   "source": [
    "Since we are dealing with image data, we will implement a Convolutional Neural Network (CNN) to classify whether twitter images are informative in the context of a humanitarian crisis or disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3) # specify the size (outer numbers) and amount (middle number) of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2) # specify pool size first number is size of pool, second is step size\n",
    "        self.conv2 = nn.Conv2d(16, 8, 3) # new depth is amount of filters in previous conv layer\n",
    "        self.fc1 = nn.Linear(54*54*8, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 2) # final fc layer needs 19 outputs because we have 19 layers # ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 54*54*8) # flatten\n",
    "        x = F.relu(self.fc1(x))    # fully connected, relu        \n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)     # output    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e102f",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Make sure to change the run_name (and 'architecture' parameter of the wandb `run` variable if necessary) with each new run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c87b",
   "metadata": {},
   "source": [
    "## Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loop\n",
    "def dev(model, val_loader): \n",
    "    model.to(device)\n",
    "    batch_size = val_loader.batch_size\n",
    "    avg = 'macro' # used when computing certain accuracy metrics\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, batch in tqdm.tqdm(enumerate(val_loader), \n",
    "                             total= len(val_loader), desc=f\"Processing validation data\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            raw_logits = model.forward(images)\n",
    "\n",
    "            preds = torch.argmax(raw_logits, dim=1) # https://discuss.pytorch.org/t/cross-entropy-loss-get-predicted-class/58215\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_trues.extend(labels.tolist())\n",
    "\n",
    "\n",
    "        # metrics \n",
    "        acc_total = accuracy_score(y_true=all_trues, y_pred=all_preds)\n",
    "        precision = precision_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        recall = recall_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        f1 = f1_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "\n",
    "        avg_eval_loss = eval_loss / (len(val_loader))\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': acc_total, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1': f1, \n",
    "            'avg_eval_loss': avg_eval_loss\n",
    "        }\n",
    "        print('****Evaluation****')\n",
    "        print(f'total_accuracy: {acc_total}')\n",
    "\n",
    "        return acc_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_eval(model, num_epochs, lr):\n",
    "    # training hyperparameters & functions/tools\n",
    "    lr = lr \n",
    "    num_epochs = num_epochs\n",
    "    \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #stocastic gradient descent for our optimization algorithm\n",
    "    lr_sched = MultiStepLR(optimizer=optimizer, milestones=list(range(50, num_epochs, 30)), gamma=.1) # decaying lr\n",
    "\n",
    "    model.to(device)\n",
    "    # for saving the models\n",
    "    #Path(research_dir, 'models' ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for b, batch in tqdm.tqdm(enumerate(train_loader), \n",
    "                            total= len(train_loader), desc=f\"Processing training data in epoch {epoch+1}\"):\n",
    "            model.train()\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            model.zero_grad() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            raw_logits = model.forward(images) # forward pass\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels) # raw logits as input\n",
    "            print(f'Train Loss: {loss}')\n",
    "            \n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # gradient update\n",
    "\n",
    "            if (b+1) % 20 == 0:\n",
    "                print(f'batch: {b+1} ; loss: {loss.item()}')\n",
    "        \n",
    "        # each epoch, run validation\n",
    "        acc = dev(model=model, val_loader=val_loader)\n",
    "        \n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            #torch.save(model, Path(research_dir, 'models', f'{run_name}'))\n",
    "        \n",
    "        lr_sched.step()\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f72e67",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "os.chdir(home)\n",
    "\n",
    "# grid-search\n",
    "history = []\n",
    "for lr in [10**-3, 10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    model = ConvNet()\n",
    "\n",
    "    acc = train_eval(model, num_epochs, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80795ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print/report final model hyperparameters and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b295cb",
   "metadata": {},
   "source": [
    "# Updated CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c68867",
   "metadata": {},
   "source": [
    "Since the previous CNN worked well, we will tweak that model architecture by adding layers to hopefully improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3) # specify the size (outer numbers) and amount (middle number) of filters\n",
    "        self.pool = nn.MaxPool2d(2, 2) # specify pool size first number is size of pool, second is step size\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3) # new depth is amount of filters in previous conv layer\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64*26*26, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 2) \n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3]) # flatten\n",
    "        x = F.relu(self.fc1(x))    # fully connected, relu         \n",
    "        x = F.relu(self.fc2(x))    \n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573b71f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee04d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "os.chdir(home)\n",
    "\n",
    "history = []\n",
    "for lr in [10**-3, 10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    model = ConvNet2()\n",
    "\n",
    "    # update architecture = (?)\n",
    "    acc = train_eval(model, num_epochs, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d07d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0729b7",
   "metadata": {},
   "source": [
    "# MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48b269",
   "metadata": {},
   "source": [
    "With disaster reports being inherently spontaneous, we would like to investigate more efficient models such as MobileNetV2 to analyze the efficiency-accuracy tradeoff for predicting informative or not informative images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a494699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Involution Block - Filter for each \"location\" in the image\n",
    "class Inv2d(nn.Module): \n",
    "    def __init__(self, in_channels, reduction=1, kernel_size=3, group_ch=1, stride=1, padding=1, dilation=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.k = kernel_size\n",
    "        self.group_ch = group_ch\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.g = in_channels // group_ch\n",
    "        self.r = reduction\n",
    "\n",
    "        self.stride=stride\n",
    "        self.padding = padding\n",
    "        self.dilation=dilation\n",
    "\n",
    "        # avg pool is for adjusting the kernel and therefore output based on the stride\n",
    "        self.o = nn.AvgPool2d(kernel_size=stride, stride=stride) if stride > 1 else nn.Identity()\n",
    "        self.reduce = nn.Conv2d(in_channels, in_channels // reduction, 1)\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels//reduction)\n",
    "        self.span = nn.Conv2d(in_channels // reduction, self.g*(kernel_size**2), 1)\n",
    "        self.unfold = nn.Unfold(kernel_size, padding=padding, stride=stride, dilation=dilation)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Input should be an image tensor of shape: (batch_size, channels, h, w)\n",
    "        '''\n",
    "        # KERNEL GENERATION\n",
    "        W = self.reduce(self.o(X))\n",
    "  \n",
    "        W = nn.ReLU()(self.batch_norm(W))\n",
    "        W = self.span(W)\n",
    " \n",
    "        b, c, h, w = W.shape\n",
    "        W = W.view(b, self.g, 1, self.k**2, w*h)\n",
    "\n",
    "        # INVOLUTION\n",
    "        # print(f'stride: {self.stride}')\n",
    "        # print(f'padding: {self.padding}')\n",
    "        \n",
    "        patches = self.unfold(X)\n",
    "        patches = patches.view(b, self.g, self.group_ch, self.k**2, w*h)\n",
    "        out = patches*W \n",
    "\n",
    "        # output width and height should be w / stride and h / stride\n",
    "        out = out.view(b, self.out_channels, self.k**2, w*h).sum(dim=2)\n",
    "        out = out.view(b, self.out_channels, w, h)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Inverted residual block - bottleneck architecture with residual connection (like resnet)\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio, norm_layer=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert stride == 1 or stride == 2, 'stride must be 1 or 2'\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        hidden_dim = int(round(in_channels * expand_ratio))\n",
    "        # print(hidden_dim)\n",
    "\n",
    "        layers = [] \n",
    "\n",
    "        if expand_ratio != 1:\n",
    "            # add expansion \n",
    "            layers.append(Conv2dNormActivation(in_channels, hidden_dim, kernel_size=1, stride=stride, \n",
    "                                               norm_layer=norm_layer, activation_layer=nn.ReLU6))\n",
    "            # depthwise conv => pointwise => norm_layer\n",
    "            # hidden_dim == in_channels if expand_ratio==1\n",
    "            # b/c groups = hidden_dim, each input channel is convolved with its own set of filters\n",
    "            # b/c instead of using multiple 3d kernels, we want multiple 2d kernels that each go across a different channel\n",
    "        layers.extend([Conv2dNormActivation(hidden_dim, hidden_dim, kernel_size=3, stride=stride, groups=hidden_dim, norm_layer=norm_layer, activation_layer=nn.ReLU6), \n",
    "                                                nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False), # point-wise (1x1 filter)\n",
    "                                                norm_layer(out_channels)])\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self._is_cn = stride > 1 # downsample indicator\n",
    "        self.use_res_conn = stride == 1 and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_conn:\n",
    "            return x + self.conv(x)\n",
    "        else:                \n",
    "            return self.conv(x)\n",
    "        \n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None): \n",
    "    '''\n",
    "    Description: rounds v to nearest multiple of divisor\n",
    "    '''\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# mobilenetv2 architecture\n",
    "class MobileNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "            block: Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer: Module specifying the normalization layer to use\n",
    "\n",
    "        \"\"\"\n",
    "    def __init__(self, width_mult=1.0, inverted_residual_setting = None, round_nearest=8,\n",
    "                  block = None, norm_layer = None, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        input_channel = 32 # adjustable\n",
    "        last_channel = 1280 # adjustable\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t - expansion factor, c - # of output after block, n - # of repitions, s - stride\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # build the first layer\n",
    "        # input_channel is the number of channels after the first Conv2dNormActivation\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        \n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        \n",
    "        features = [Conv2dNormActivation(3, input_channel, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.ReLU6)]\n",
    "\n",
    "        # build the inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c*width_mult, round_nearest) # update output channels (will increase)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
    "                input_channel = output_channel \n",
    "                \n",
    "        # build last severals\n",
    "        features.append(Conv2dNormActivation(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer, activation_layer=nn.ReLU6))\n",
    "\n",
    "        # complete CNN architecture\n",
    "        self.features = nn.Sequential(*features)  \n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            # print('initializing weights')\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def _forward_impl(self, x):\n",
    "        x = self.features(x) # pass input through layers\n",
    "        \n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)) # takes the average value for each each channel\n",
    "        x = torch.flatten(x, 1) # flatten for linear layer\n",
    "        # x = self.classifier(x) # classify\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8a398b",
   "metadata": {},
   "source": [
    "## Transfer Learning with MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ae34b",
   "metadata": {},
   "source": [
    "We see if we can improve performance and efficiency of the MobileNetV2 model by using transfer learning with `weights` = 'IMAGENET1K_V2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "history = []\n",
    "for lr in [10**-2, 10**-1, 30**-1, 1]:\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "\n",
    "    # instantiate our model\n",
    "    model = MobileNetV2()\n",
    "\n",
    "    # load pretrained weights\n",
    "    pretrained = mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "    weights = pretrained.state_dict()\n",
    "    model.features.load_state_dict(weights, strict=False)\n",
    "\n",
    "    # turn off all but the topmost layers\n",
    "    freeze_up_to = 9\n",
    "    for param in model.features[:freeze_up_to].parameters(): \n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = nn.Sequential(model, nn.Linear(1280, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 2)) # 1280 is the num of features outputted by MobileNetv2 after flattening\n",
    "    \n",
    "    acc = train_eval(model, num_epochs, run_name, lr=lr)\n",
    "    my_dict['acc'] = acc\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final mobilenet metrics, is it better than the updated CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5db65",
   "metadata": {},
   "source": [
    "# Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b62709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad cam implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34b15a",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94407410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model choice and analysis on metrics (compared to baseline model or existing models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f525e32",
   "metadata": {},
   "source": [
    "# Contribution From Group Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db22f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stuff as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d45f",
   "metadata": {},
   "source": [
    "Agafia's Contribution: Implemented grid-search to optimize hyperparameters and transfer learning for MobileNetV2. Implemented involution class. Wrote code to connect to SQL server and build dataset in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fea653",
   "metadata": {},
   "source": [
    "Ethan's Contribution: Implemented and commented on ConvNet class and training/validation loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8713ec",
   "metadata": {},
   "source": [
    "Hudson's Contribution: Implemented and commented on MobileNetV2, InvertedResidual classes. Created final ipynb file."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
