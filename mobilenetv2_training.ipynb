{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e60e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary solution to crashing\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b168c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model_helper import MobileNetV2, Inv2d\n",
    "import wandb\n",
    "import mysql.connector as connector\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import torchvision\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bdb1304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "home = os.path.expanduser('~')\n",
    "os.chdir(home) # b/c we will be using universal paths\n",
    "\n",
    "host = '127.0.0.1'\n",
    "user = 'root' # change to your username\n",
    "password = 'vasya1' # change to your password\n",
    "database = 'ai_proj_2025' # we should all have this as the db name \n",
    "\n",
    "try:\n",
    "    conn = connector.connect(\n",
    "        host = host, \n",
    "        user = user, \n",
    "        password = password, \n",
    "        database = database\n",
    "    )\n",
    "    print('success')\n",
    "except connector.Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131056b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0330c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# research_dir = Path(home, 'Desktop', 'Education', 'Spring 2025', 'AI', 'research')\n",
    "research_dir = Path(home, \"ai_research_proj_spring_2025\", \"research_ai_class_spring_2025\") # in lab 409 computer for Agafia\n",
    "os.chdir(research_dir)\n",
    "\n",
    "from data_helper import SQLDataset_Informative\n",
    "\n",
    "os.chdir(home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf83da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "# transforms\n",
    "transformations = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True), \n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1acb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val, and test sets\n",
    "\n",
    "data_dir=Path('OneDrive - Stephen F. Austin State University', 'CrisisMMD_v2.0','CrisisMMD_v2.0')\n",
    "\n",
    "train_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_train=True)\n",
    "val_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_val=True)\n",
    "test_set = SQLDataset_Informative(conn=conn, img_col='image_path', label_col='image_info', transform=transformations, \n",
    "                     data_dir=data_dir, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22567c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128)\n",
    "val_loader = DataLoader(val_set, batch_size=128)\n",
    "test_loader = DataLoader(test_set, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47139380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing accuracy metric functions\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07726630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MobileNetV2()\n",
    "# model = nn.Sequential(model, nn.Linear(1280, 2))\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'batchnorm' in name.lower() or 'relu' in name.lower():\n",
    "#         param.requires_grad = False\n",
    "#         print(f\"Freezing parameter: {name}\")\n",
    "\n",
    "# count = 0\n",
    "# params = [param for param in model.parameters()]\n",
    "# names = [name for name in model.modules()]\n",
    "\n",
    "# for name, param in zip(names, params):\n",
    "#     print(name)\n",
    "#     print(param)\n",
    "#     print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b10b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_residual_setting = [\n",
    "                # t - expansion factor, c - # of output after block, n - # of repitions, s - stride\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "                [6, 640, 1, 1], \n",
    "            ]\n",
    "\n",
    "# inv2d layers to append\n",
    "# inv2d_features = nn.Sequential(\n",
    "#     Inv2d(640, )\n",
    "# \n",
    "# )\n",
    "\n",
    "model_test = nn.Sequential(MobileNetV2(inverted_residual_setting=inverted_residual_setting))\n",
    "\n",
    "test_data = torch.randn(2, 3, 512, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inv2dModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, num_blocks, num_classes, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.activation = activation\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        \n",
    "        features = []\n",
    "\n",
    "        # make initial block\n",
    "        init_block = [nn.Conv2d(self.in_channels, self.hidden_dim, 3), nn.BatchNorm2d(self.hidden_dim), nn.ReLU()]\n",
    "        features.extend(init_block)\n",
    "        # make blocks\n",
    "        for b in range(self.num_blocks):\n",
    "            features.extend([Inv2d(self.hidden_dim), \n",
    "                             nn.BatchNorm2d(self.hidden_dim), nn.ReLU()])\n",
    "\n",
    "        self.features = nn.Sequential(*features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.features(X)\n",
    "        X = X.flatten(1, -1)\n",
    "        X = nn.Linear(X.shape[1], self.num_classes)(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59cf6640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inv2dModel(\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv2d_model = Inv2dModel(3, 64, 3, 2)\n",
    "\n",
    "inv2d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d369ac3",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Make sure to change the run_name (and 'architecture' parameter of the wandb `run` variable if necessary) with each new run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edb3f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation fn\n",
    "\n",
    "def dev(model, val_loader):\n",
    "    model.to(device)\n",
    "    batch_size = val_loader.batch_size\n",
    "    avg = 'macro' # used when computing certain accuracy metrics\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, batch in tqdm.tqdm(enumerate(val_loader), \n",
    "                             total= len(val_loader), desc=f\"Processing validation data\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            raw_logits = model.forward(images)\n",
    "\n",
    "            preds = torch.argmax(raw_logits, dim=1) # https://discuss.pytorch.org/t/cross-entropy-loss-get-predicted-class/58215\n",
    "            print(preds)\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_trues.extend(labels.tolist())\n",
    "\n",
    "\n",
    "        # metrics \n",
    "        acc_total = accuracy_score(y_true=all_trues, y_pred=all_preds)\n",
    "        precision = precision_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        recall = recall_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "        f1 = f1_score(y_true=all_trues, y_pred=all_preds, zero_division=0, average=avg)\n",
    "\n",
    "        avg_eval_loss = eval_loss / (len(val_loader))\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': acc_total, \n",
    "            'precision': precision, \n",
    "            'recall': recall, \n",
    "            'f1': f1, \n",
    "            'avg_eval_loss': avg_eval_loss\n",
    "        }\n",
    "        wandb.log(metrics)\n",
    "        print('****Evaluation****')\n",
    "        print(f'total_accuracy: {acc_total}')\n",
    "\n",
    "        return acc_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54ec2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, num_epochs, run_name, lr, architecture):\n",
    "    # training hyperparameters & functions/tools\n",
    "    lr = lr \n",
    "    num_epochs = num_epochs\n",
    "    run_name = run_name\n",
    "    \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #stocastic gradient descent for our optimization algorithm\n",
    "\n",
    "    model.to(device)\n",
    "    # for saving the models\n",
    "    Path(research_dir, 'models' ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # before training, set up wandb for tracking purposes\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"5a08d1ebbf0e86ab877a128b98be3c320301b6a0\"\n",
    "\n",
    "    run = wandb.init(\n",
    "        # Set the wandb entity where your project will be logged (generally your team name).\n",
    "        entity=\"agafiabschool-stephen-f-austin-state-university\",\n",
    "        # Set the wandb project where this run will be logged.\n",
    "        project=\"Research Project for CSCI-1465\",\n",
    "        # Track hyperparameters and run metadata.\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": architecture,\n",
    "            \"dataset\": \"CrisisMMD\",\n",
    "            \"epochs\": num_epochs,\n",
    "        }, name=run_name\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        wandb.log({'epoch': epoch+1})\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for b, batch in tqdm.tqdm(enumerate(train_loader), \n",
    "                            total= len(train_loader), desc=f\"Processing training data in epoch {epoch+1}\"):\n",
    "            model.train()\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            model.zero_grad() \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            raw_logits = model.forward(images)\n",
    "            # loss - can use raw logits for this function b/c it applies LogSoftmax \n",
    "            loss = nn.CrossEntropyLoss()(raw_logits, labels)\n",
    "\n",
    "            wandb.log({'Train Loss': loss.item()})\n",
    "\n",
    "            # backprop!\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b+1) % 10 == 0:\n",
    "                print(f'batch: {b+1} ; loss: {loss.item()}')\n",
    "        \n",
    "        # each epoch, run validation\n",
    "        acc = dev(model=model, val_loader=val_loader)\n",
    "        \n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            torch.save(model, Path(research_dir, 'models', f'{run_name}'))\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38971e32",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# model = MobileNetV2()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model = nn.Sequential(model, nn.Linear(1280, 2)) # 1280 is the num of features outputted by MobileNetv2 after flattening\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m Inv2dModel(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m my_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_eval(model, num_epochs, run_name, lr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInv2d Test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(my_dict)\n",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m, in \u001b[0;36mtrain_eval\u001b[1;34m(model, num_epochs, run_name, lr, architecture)\u001b[0m\n\u001b[0;32m      5\u001b[0m run_name \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m      8\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr) \u001b[38;5;66;03m#stocastic gradient descent for our optimization algorithm\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# for saving the models\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bowdenaa\\AppData\\Local\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\adam.py:78\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     67\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     68\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     77\u001b[0m )\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\bowdenaa\\AppData\\Local\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:366\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    364\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    368\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# time to train\n",
    "num_epochs = 50\n",
    "os.chdir(home)\n",
    "\n",
    "# for grid-search\n",
    "history = []\n",
    "for lr in [10**-4, 30**-4, 10**-3, 30**-3, 10**-2, 30**-2, 10**-1, 30**-1, 1]:\n",
    "    # run_name = f'MobileNetV2 lr={lr}'\n",
    "    run_name = f'Inv2d lr={lr}'\n",
    "    my_dict = {}\n",
    "    my_dict['lr'] = lr\n",
    "    # model = MobileNetV2()\n",
    "    # model = nn.Sequential(model, nn.Linear(1280, 2)) # 1280 is the num of features outputted by MobileNetv2 after flattening\n",
    "    model = Inv2dModel(3, 64, 3, 2)\n",
    "    my_dict['acc'] = train_eval(model, num_epochs, run_name, lr, 'Inv2d Test')\n",
    "    history.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd95913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
